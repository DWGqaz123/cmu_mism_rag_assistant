# utils/text_processing/mism_doc_loader.py
import re
from typing import List, Tuple
from langchain.schema import Document
import os
from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredHTMLLoader
from langchain.docstore.document import Document
from typing import List
from .category_map import category_map  

def filter_documents(documents: List[Document], min_length: int = 100) -> Tuple[List[Document], List[Document]]:
    """
    æ¸…æ´—æ–‡æ¡£ï¼šè¿‡æ»¤å†…å®¹è¿‡çŸ­æˆ–åŒ…å«ä¹±ç çš„ Document å®ä¾‹ã€‚

    Args:
        documents (List[Document]): åŸå§‹æ–‡æ¡£åˆ—è¡¨ã€‚
        min_length (int): æ–‡æ¡£å†…å®¹çš„æœ€å°æœ‰æ•ˆå­—ç¬¦æ•°ï¼Œé»˜è®¤100ã€‚

    Returns:
        Tuple[List[Document], List[Document]]: 
            (clean_documents, filtered_documents)
            - clean_documents: æ¸…æ´—åçš„æœ‰æ•ˆæ–‡æ¡£åˆ—è¡¨ï¼›
            - filtered_documents: è¢«å‰”é™¤çš„æ— æ•ˆ/ä¹±ç æ–‡æ¡£åˆ—è¡¨ã€‚
    """
    clean_documents = []
    filtered_documents = []

    for doc in documents:
        content = doc.page_content.strip()

        # æ¡ä»¶1ï¼šå†…å®¹è¿‡çŸ­
        too_short = len(content) < min_length

        # æ¡ä»¶2ï¼šç–‘ä¼¼ä¹±ç ï¼ˆå¦‚è¿ç»­20ä¸ªéå­—æ¯æ•°å­—ç©ºæ ¼ï¼‰
        has_garbage = re.search(r'[^a-zA-Z0-9\s]{20,}', content[:500]) is not None

        if too_short or has_garbage:
            filtered_documents.append(doc)
        else:
            clean_documents.append(doc)
    
    print(f"\nğŸ“Š æ–‡æ¡£æ¸…æ´—å®Œæˆï¼š")
    print(f"  - åŸå§‹æ–‡æ¡£æ•°é‡ï¼š{len(documents)}")
    print(f"  - ä¿ç•™æœ‰æ•ˆæ–‡æ¡£ï¼š{len(clean_documents)}")
    print(f"  - å‰”é™¤æ— æ•ˆ/ä¹±ç æ–‡æ¡£ï¼š{len(filtered_documents)}")

    return clean_documents, filtered_documents


# æŒ‰ç±»åˆ«åŠ è½½æ–‡æ¡£
def load_documents_with_category(base_path: str) -> List[Document]:
    all_docs = []

    for root, dirs, files in os.walk(base_path):
        for file in files:
            full_path = os.path.join(root, file)
            rel_path = os.path.relpath(full_path, base_path).replace("\\", "/")  # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦

            # é€‰æ‹©åˆé€‚çš„ Loader
            if file.endswith(".txt"):
                loader = TextLoader(full_path, autodetect_encoding=True)
            elif file.endswith(".pdf"):
                loader = PyPDFLoader(full_path)
            elif file.endswith(".html"):
                loader = UnstructuredHTMLLoader(full_path)
            else:
                continue

            try:
                raw_docs = loader.load()
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡åŠ è½½å¤±è´¥æ–‡ä»¶ï¼š{rel_path}ï¼Œé”™è¯¯ï¼š{e}")
                continue

            # ä¸ºæ¯ä¸ª document æ·»åŠ  metadata
            category = category_map.get(rel_path, category_map.get(rel_path.split("/")[0], "unknown"))
            for doc in raw_docs:
                doc.metadata["source"] = rel_path
                doc.metadata["category"] = category

            all_docs.extend(raw_docs)

    # æ¸…æ´—æ–‡æ¡£
    clean_docs, filtered_docs = filter_documents(all_docs)
    return clean_docs