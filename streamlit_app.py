# streamlit_app.py

# ==============================================================================
# 1. å¿…è¦çš„åº“å¯¼å…¥
# ==============================================================================
import streamlit as st
import os
import re
import torch
import json
import time
from datetime import datetime

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, UnstructuredHTMLLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# è¯­ä¹‰åˆ†å—å™¨
from langchain_experimental.text_splitter import SemanticChunker

# é‡æ’åºæ¨¡å‹
from sentence_transformers import CrossEncoder

# BM25 ç¨€ç–æ£€ç´¢
from rank_bm25 import BM25Okapi

# LangChain Document ç±»
from langchain.docstore.document import Document

# å¯¼å…¥è‡ªå®šä¹‰å·¥å…·å‡½æ•°
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'utils')))

# å¯¼å…¥è‡ªå®šä¹‰å·¥å…·
try:
    from utils.load_key import load_key
    from utils.text_processing.query_classifier import classify_query, CATEGORIES
    from utils.text_processing.mism_doc_loader import filter_documents
    from utils.text_processing.category_map import category_map
    from utils.model_training.train_eval_utils import set_seed
except ImportError as e:
    st.error(f"âŒ æ— æ³•å¯¼å…¥è‡ªå®šä¹‰å·¥å…·æ¨¡å—ï¼š{e}ã€‚è¯·æ£€æŸ¥ 'utils' ç›®å½•åŠå…¶å­ç›®å½•æ˜¯å¦å­˜åœ¨äºæ­£ç¡®è·¯å¾„ã€‚")
    st.stop()

# ==============================================================================
# 2. é¡µé¢é…ç½®å’Œæ ·å¼è®¾ç½®
# ==============================================================================

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="CMU MISM RAG Assistant", 
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    /* ä¸»é¢˜è‰²å½©å®šä¹‰ */
    :root {
        --cmu-red: #c41e3a;
        --cmu-dark-red: #8b0000;
        --cmu-gold: #ffb81c;
        --dark-bg: #0e1117;
        --card-bg: #1e1e1e;
        --text-primary: #ffffff;
        --text-secondary: #b0b0b0;
        --accent-blue: #4a90e2;
        --success-green: #00d4aa;
        --warning-orange: #ff6b35;
    }

    /* éšè—é»˜è®¤çš„Streamlitæ ·å¼ */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* ä¸»å®¹å™¨æ ·å¼ */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
        max-width: 1200px;
    }
    
    /* æ ‡é¢˜åŒºåŸŸç¾åŒ– */
    .main-header {
        background: linear-gradient(135deg, var(--cmu-red) 0%, var(--cmu-dark-red) 100%);
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        box-shadow: 0 8px 32px rgba(196, 30, 58, 0.3);
        text-align: center;
        color: white;
    }
    
    .main-header h1 {
        font-size: 3rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    }
    
    .main-header p {
        font-size: 1.2rem;
        opacity: 0.9;
        margin-bottom: 0;
    }
    
    /* å¡ç‰‡æ ·å¼ */
    .info-card {
        background: var(--card-bg);
        padding: 1.5rem;
        border-radius: 12px;
        border: 1px solid rgba(255, 255, 255, 0.1);
        margin-bottom: 1rem;
        box-shadow: 0 4px 16px rgba(0, 0, 0, 0.2);
    }
    
    /* èŠå¤©æ¶ˆæ¯æ ·å¼ */
    .chat-message {
        padding: 1rem;
        border-radius: 12px;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    .user-message {
        background: linear-gradient(135deg, var(--accent-blue) 0%, #357abd 100%);
        color: white;
        margin-left: 2rem;
    }
    
    .assistant-message {
        background: var(--card-bg);
        border: 1px solid rgba(255, 255, 255, 0.1);
        margin-right: 2rem;
    }
    
    /* çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-indicator {
        display: inline-flex;
        align-items: center;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-size: 0.9rem;
        font-weight: 500;
        margin: 0.25rem;
    }
    
    .status-success {
        background: rgba(0, 212, 170, 0.2);
        color: var(--success-green);
        border: 1px solid var(--success-green);
    }
    
    .status-info {
        background: rgba(74, 144, 226, 0.2);
        color: var(--accent-blue);
        border: 1px solid var(--accent-blue);
    }
    
    .status-warning {
        background: rgba(255, 107, 53, 0.2);
        color: var(--warning-orange);
        border: 1px solid var(--warning-orange);
    }
    
    /* ä¾§è¾¹æ ç¾åŒ– */
    .sidebar .sidebar-content {
        background: var(--card-bg);
        border-radius: 12px;
        padding: 1rem;
    }
    
    /* æŒ‰é’®ç¾åŒ– */
    .stButton > button {
        background: linear-gradient(135deg, var(--cmu-red) 0%, var(--cmu-dark-red) 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 12px rgba(196, 30, 58, 0.3);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(196, 30, 58, 0.4);
    }
    
    /* å±•å¼€å™¨ç¾åŒ– */
    .streamlit-expanderHeader {
        background: var(--card-bg);
        border-radius: 8px;
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    
    /* æ–‡æœ¬åŒºåŸŸç¾åŒ– */
    .stTextArea > div > div > textarea {
        background: var(--card-bg);
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 8px;
        color: var(--text-primary);
    }
    
    /* è¿›åº¦æ¡ç¾åŒ– */
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, var(--cmu-red) 0%, var(--cmu-gold) 100%);
    }
    
    /* æŒ‡æ ‡å¡ç‰‡ */
    .metric-card {
        background: var(--card-bg);
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid rgba(255, 255, 255, 0.1);
        text-align: center;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        color: var(--cmu-red);
        margin-bottom: 0.5rem;
    }
    
    .metric-label {
        font-size: 0.9rem;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }
    
    /* åŠ¨ç”»æ•ˆæœ */
    @keyframes fadeInUp {
        from {
            opacity: 0;
            transform: translateY(20px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    .fade-in-up {
        animation: fadeInUp 0.6s ease-out;
    }
    
    /* å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {
        .main-header h1 {
            font-size: 2rem;
        }
        
        .main-header p {
            font-size: 1rem;
        }
        
        .user-message {
            margin-left: 0;
        }
        
        .assistant-message {
            margin-right: 0;
        }
    }
</style>
""", unsafe_allow_html=True)

# ==============================================================================
# 3. RAG ç»„ä»¶åŠ è½½ä¸ç¼“å­˜
# ==============================================================================
@st.cache_resource
def load_rag_components():
    """åŠ è½½å¹¶åˆå§‹åŒ–æ‰€æœ‰RAGç»„ä»¶"""
    
    # åˆ›å»ºåŠ è½½è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    def update_progress(value, text):
        progress_bar.progress(value)
        status_text.markdown(f"<div class='status-indicator status-info'>ğŸ”„ {text}</div>", unsafe_allow_html=True)
    
    update_progress(0.1, "åˆå§‹åŒ–è®¾å¤‡å’Œç¯å¢ƒ...")
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
    set_seed(66)
    
    update_progress(0.2, "åŠ è½½APIå¯†é’¥...")
    
    # APIå¯†é’¥åŠ è½½
    openai_api_key_val = os.environ.get("OPENAI_API_KEY")
    
    if not openai_api_key_val:
        if "OPENAI_API_KEY" in st.secrets:
            openai_api_key_val = st.secrets["OPENAI_API_KEY"]
        else:
            keys_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), 'keys.json'))
            if os.path.exists(keys_file_path):
                try:
                    with open(keys_file_path, "r", encoding="utf-8") as f:
                        keys_data = json.load(f)
                        if "OPENAI_API_KEY" in keys_data and keys_data["OPENAI_API_KEY"]:
                            openai_api_key_val = keys_data["OPENAI_API_KEY"]
                except Exception as e:
                    st.error(f"âŒ ä» 'keys.json' åŠ è½½å¯†é’¥å¤±è´¥ï¼š{e}")
                    st.stop()
            else:
                st.error("âŒ 'OPENAI_API_KEY' æœªè®¾ç½®")
                st.stop()
    
    if not openai_api_key_val:
        st.error("âŒ æ— æ³•è·å– OpenAI API å¯†é’¥")
        st.stop()
    
    update_progress(0.3, "åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
    
    # LLMåˆå§‹åŒ–
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=openai_api_key_val)
    
    update_progress(0.4, "åŠ è½½åµŒå…¥æ¨¡å‹...")
    
    # åµŒå…¥æ¨¡å‹
    embeddings_model_name = "all-MiniLM-L6-v2"
    embeddings = HuggingFaceEmbeddings(
        model_name=embeddings_model_name,
        model_kwargs={'device': str(device)}
    )
    
    update_progress(0.5, "è®¾ç½®é—®ç­”é“¾...")
    
    # QAé“¾è®¾ç½®
    QA_CHAIN_PROMPT = ChatPromptTemplate.from_template("""You are an assistant that answers questions only based on provided context about the CMU MISM program.
When the question is about application requirements, look specifically for information in admissions pages or files and **provide a comprehensive and detailed list of all relevant requirements**.
When the question is about career paths, refer to student handbooks or career services sections and **describe them thoroughly**.
Do not infer from general knowledge if the answer is not in the documents.

{context}

Question: {question}
Helpful Answer:""")
    
    flexible_qa_chain = (
        QA_CHAIN_PROMPT
        | llm
        | StrOutputParser()
    )
    
    update_progress(0.6, "åŠ è½½é‡æ’åºæ¨¡å‹...")
    
    # é‡æ’åºæ¨¡å‹
    try:
        reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    except Exception as e:
        st.error(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        st.stop()
    
    update_progress(0.7, "åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£...")
    
    # çŸ¥è¯†åº“æ–‡æ¡£åŠ è½½
    KNOWLEDGE_BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), 'data', 'cmu_mism_docs'))
    CHROMA_PERSIST_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), 'vectorstore', 'cmu_mism_chroma'))
    
    documents = []
    
    def load_and_tag_documents(path, glob_pattern, loader_cls, category_key):
        doc_list = []
        if os.path.exists(path):
            loader = DirectoryLoader(path, glob=glob_pattern, loader_cls=loader_cls)
            try:
                raw_docs = loader.load()
                for doc in raw_docs:
                    rel_path = os.path.relpath(doc.metadata.get('source', ''), KNOWLEDGE_BASE_DIR).replace("\\", "/")
                    doc.metadata["source"] = rel_path
                    doc.metadata["category"] = category_map.get(rel_path, category_map.get(category_key, "unknown"))
                return raw_docs
            except Exception as e:
                return []
        return []
    
    # åŠ è½½å„ç±»æ–‡æ¡£
    html_docs_path = os.path.join(KNOWLEDGE_BASE_DIR, 'websites_pages')
    html_docs = load_and_tag_documents(html_docs_path, "**/*.html", UnstructuredHTMLLoader, 'websites_pages')
    documents.extend(html_docs)
    
    pdf_docs_path = os.path.join(KNOWLEDGE_BASE_DIR, 'handbooks')
    pdf_docs = load_and_tag_documents(pdf_docs_path, "**/*.pdf", PyPDFLoader, 'handbooks')
    documents.extend(pdf_docs)
    
    forums_txt_path = os.path.join(KNOWLEDGE_BASE_DIR, 'forums')
    forums_txt_docs = load_and_tag_documents(forums_txt_path, "**/*.txt", TextLoader, 'forums')
    documents.extend(forums_txt_docs)
    
    text_info_path = os.path.join(KNOWLEDGE_BASE_DIR, 'text_info')
    text_info_docs = load_and_tag_documents(text_info_path, "**/*.txt", TextLoader, 'text_info')
    documents.extend(text_info_docs)
    
    # æ–‡æ¡£æ¸…æ´—
    documents, _ = filter_documents(documents)
    
    update_progress(0.8, "æ‰§è¡Œè¯­ä¹‰åˆ†å—...")
    
    # è¯­ä¹‰åˆ†å—
    def simple_tokenizer(text):
        return re.findall(r'\w+', text.lower())
    
    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    chunks = text_splitter.split_documents(documents)
    
    update_progress(0.85, "æ„å»ºBM25ç´¢å¼•...")
    
    # BM25ç´¢å¼•
    tokenized_corpus = [simple_tokenizer(doc.page_content) for doc in chunks]
    bm25_index = BM25Okapi(tokenized_corpus)
    bm25_doc_list = chunks
    
    update_progress(0.9, "æ„å»ºå‘é‡æ•°æ®åº“...")
    
    # å‘é‡æ•°æ®åº“
    try:
        if os.path.exists(CHROMA_PERSIST_DIR) and len(os.listdir(CHROMA_PERSIST_DIR)) > 0:
            vectordb = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings)
            try:
                _ = vectordb.similarity_search("test", k=1)
            except Exception as e:
                import shutil
                shutil.rmtree(CHROMA_PERSIST_DIR)
                os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)
                vectordb = Chroma.from_documents(
                    documents=chunks,
                    embedding=embeddings,
                    persist_directory=CHROMA_PERSIST_DIR
                )
        else:
            os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)
            vectordb = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings,
                persist_directory=CHROMA_PERSIST_DIR
            )
    except Exception as e:
        st.error(f"âŒ å‘é‡æ•°æ®åº“æ“ä½œå¤±è´¥ï¼š{e}")
        st.stop()
    
    update_progress(1.0, "å®Œæˆåˆå§‹åŒ–!")
    
    # æ¸…ç†è¿›åº¦æŒ‡ç¤ºå™¨
    time.sleep(1)
    progress_bar.empty()
    status_text.empty()
    
    return llm, embeddings, QA_CHAIN_PROMPT, flexible_qa_chain, vectordb, bm25_index, bm25_doc_list, reranker_model, simple_tokenizer, len(documents), len(chunks)

# ==============================================================================
# 4. RAG æ ¸å¿ƒé€»è¾‘å‡½æ•°
# ==============================================================================
def get_rag_answer(question: str, vectordb, llm, flexible_qa_chain, bm25_index, bm25_doc_list, reranker_model, simple_tokenizer_func):
    """æ‰§è¡ŒRAGæµç¨‹å¹¶è¿”å›ç­”æ¡ˆ"""
    response_metadata = {}
    
    # æŸ¥è¯¢åˆ†ç±»
    predicted_category = classify_query(question)
    response_metadata['query_category'] = predicted_category
    
    # æ„å»ºè¿‡æ»¤æ¡ä»¶
    filter_condition = {}
    if predicted_category != "unknown":
        filter_condition = {"category": predicted_category}
    
    # æ··åˆæ£€ç´¢
    vector_results = vectordb.similarity_search(
        query=question,
        k=10,
        filter=filter_condition
    )
    
    # BM25æ£€ç´¢
    tokenized_query = simple_tokenizer_func(question)
    bm25_results = []
    if tokenized_query:
        doc_scores = bm25_index.get_scores(tokenized_query)
        sorted_doc_indices_and_scores = sorted(enumerate(doc_scores), key=lambda x: x[1], reverse=True)
        top_n_bm25 = 10
        for i, (doc_original_index, score_value) in enumerate(sorted_doc_indices_and_scores):
            if i >= top_n_bm25:
                break
            bm25_results.append(bm25_doc_list[doc_original_index])
    
    # åˆå¹¶å’Œå»é‡
    unique_docs_map = {}
    for doc in vector_results:
        unique_docs_map[doc.page_content] = doc
    for doc in bm25_results:
        if doc.page_content not in unique_docs_map:
            unique_docs_map[doc.page_content] = doc
    retrieved_docs = list(unique_docs_map.values())
    
    response_metadata['retrieved_doc_count_before_rerank'] = len(retrieved_docs)
    
    # é‡æ’åº
    if retrieved_docs:
        pairs = [(question, doc.page_content) for doc in retrieved_docs]
        rerank_scores = reranker_model.predict(pairs)
        ranked_docs_with_scores = sorted(zip(retrieved_docs, rerank_scores), key=lambda x: x[1], reverse=True)
        retrieved_docs = [doc for doc, score in ranked_docs_with_scores]
    
    response_metadata['retrieved_doc_count_after_rerank'] = len(retrieved_docs)
    
    # å‡†å¤‡ä¸Šä¸‹æ–‡
    context_strings = [doc.page_content for doc in retrieved_docs]
    context_str = "\n\n".join(context_strings)
    
    # è°ƒç”¨LLM
    if not context_str.strip():
        response_content = "æŠ±æ­‰ï¼Œæ ¹æ®æˆ‘çš„çŸ¥è¯†åº“ï¼Œæœªèƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        source_docs_info = []
    else:
        response_content = flexible_qa_chain.invoke({
            "context": context_str,
            "question": question
        })
        source_docs_info = retrieved_docs
    
    return response_content, source_docs_info, response_metadata, context_str

# ==============================================================================
# 5. ä¸»ç•Œé¢
# ==============================================================================

# ä¸»æ ‡é¢˜åŒºåŸŸ
st.markdown("""
<div class="main-header fade-in-up">
    <h1>ğŸ“ CMU MISM RAG æ™ºèƒ½é—®ç­”åŠ©æ‰‹</h1>
    <p>æ¬¢è¿ä½¿ç”¨å¡å†…åŸºæ¢…éš†å¤§å­¦ä¿¡æ¯ç³»ç»Ÿç®¡ç†é¡¹ç›®æ™ºèƒ½é—®ç­”ç³»ç»Ÿ</p>
</div>
""", unsafe_allow_html=True)

# ä¾§è¾¹æ 
with st.sidebar:
    st.markdown("### ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    
    # åŠ è½½RAGç»„ä»¶
    with st.spinner("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ..."):
        try:
            llm, embeddings, QA_CHAIN_PROMPT, flexible_qa_chain, vectordb, bm25_index, bm25_doc_list, reranker_model, simple_tokenizer_func, total_docs, total_chunks = load_rag_components()
            
            # æ˜¾ç¤ºç³»ç»ŸæŒ‡æ ‡
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{total_docs}</div>
                    <div class="metric-label">æ–‡æ¡£æ€»æ•°</div>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value">{total_chunks}</div>
                    <div class="metric-label">æ–‡æ¡£å—æ•°</div>
                </div>
                """, unsafe_allow_html=True)
            
            st.markdown("<div class='status-indicator status-success'>âœ… ç³»ç»Ÿå°±ç»ª</div>", unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            st.stop()
    
    st.markdown("---")
    st.markdown("### ğŸ“š çŸ¥è¯†åº“è¦†ç›–")
    st.markdown("""
    - ğŸŒ **å®˜æ–¹ç½‘ç«™**: é¡¹ç›®ä»‹ç»ã€è¯¾ç¨‹ä¿¡æ¯
    - ğŸ“– **å­¦ç”Ÿæ‰‹å†Œ**: è¯¦ç»†æŒ‡å¯¼æ–‡æ¡£
    - ğŸ’¬ **è®ºå›è®¨è®º**: å­¦ç”Ÿç»éªŒåˆ†äº«
    - ğŸ“„ **æ–‡æœ¬èµ„æ–™**: è¡¥å……ä¿¡æ¯ææ–™
    """)
    
    st.markdown("---")
    st.markdown("### ğŸ’¡ ä½¿ç”¨å»ºè®®")
    st.markdown("""
    - è¯¢é—®**ç”³è¯·è¦æ±‚**å’Œ**æˆªæ­¢æ—¥æœŸ**
    - äº†è§£**è¯¾ç¨‹è®¾ç½®**å’Œ**å­¦ä¹ å†…å®¹**
    - æ¢ç´¢**å°±ä¸šå‰æ™¯**å’Œ**èŒä¸šå‘å±•**
    - æŸ¥è¯¢**å­¦è´¹è´¹ç”¨**å’Œ**å¥–å­¦é‡‘**
    """)

# ä¸»èŠå¤©ç•Œé¢
# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state['message_count'] = 0

# æ˜¾ç¤ºæ¬¢è¿æ¶ˆæ¯
if len(st.session_state.messages) == 0:
    st.markdown("""
    <div class="info-card fade-in-up">
        <h3>ğŸ‘‹ æ¬¢è¿ä½¿ç”¨CMU MISMæ™ºèƒ½åŠ©æ‰‹</h3>
        <p>æˆ‘å¯ä»¥å¸®åŠ©æ‚¨äº†è§£å¡å†…åŸºæ¢…éš†å¤§å­¦ä¿¡æ¯ç³»ç»Ÿç®¡ç†ç¡•å£«é¡¹ç›®çš„ç›¸å…³ä¿¡æ¯ã€‚è¯·éšæ—¶å‘æˆ‘æé—®ï¼</p>
        <p><strong>ç¤ºä¾‹é—®é¢˜ï¼š</strong></p>
        <ul>
            <li>CMU MISMé¡¹ç›®çš„ç”³è¯·è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ</li>
            <li>è¿™ä¸ªé¡¹ç›®çš„è¯¾ç¨‹è®¾ç½®å¦‚ä½•ï¼Ÿ</li>
            <li>æ¯•ä¸šåçš„å°±ä¸šå‰æ™¯æ€æ ·ï¼Ÿ</li>
            <li>å­¦è´¹å’Œç”Ÿæ´»è´¹å¤§æ¦‚éœ€è¦å¤šå°‘ï¼Ÿ</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

# æ˜¾ç¤ºèŠå¤©å†å²
for i, message in enumerate(st.session_state.messages):
    if message["role"] == "user":
        st.markdown(f"""
        <div class="chat-message user-message fade-in-up">
            <strong>ğŸ™‹â€â™‚ï¸ æ‚¨ï¼š</strong><br>
            {message["content"]}
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chat-message assistant-message fade-in-up">
            <strong>ğŸ¤– åŠ©æ‰‹ï¼š</strong><br>
            {message["content"]}
        </div>
        """, unsafe_allow_html=True)

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("ğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.markdown(f"""
    <div class="chat-message user-message fade-in-up">
        <strong>ğŸ™‹â€â™‚ï¸ æ‚¨ï¼š</strong><br>
        {prompt}
    </div>
    """, unsafe_allow_html=True)
    
    # ç”Ÿæˆå›ç­”
    with st.spinner("ğŸ¤” æ­£åœ¨æ€è€ƒä¸­..."):
        try:
            answer_content, source_docs_for_ui, metadata_info, retrieved_context_str = get_rag_answer(
                prompt, vectordb, llm, flexible_qa_chain, bm25_index, bm25_doc_list, reranker_model, simple_tokenizer_func
            )
            
            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            st.session_state.messages.append({"role": "assistant", "content": answer_content})
            
            # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
            st.markdown(f"""
            <div class="chat-message assistant-message fade-in-up">
                <strong>ğŸ¤– åŠ©æ‰‹ï¼š</strong><br>
                {answer_content}
            </div>
            """, unsafe_allow_html=True)
            
            # æ˜¾ç¤ºå¤„ç†è¯¦æƒ…
            with st.expander("ğŸ” å¤„ç†è¯¦æƒ…", expanded=False):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-value">{metadata_info.get('query_category', 'N/A')}</div>
                        <div class="metric-label">æŸ¥è¯¢ç±»åˆ«</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-value">{metadata_info.get('retrieved_doc_count_before_rerank', 'N/A')}</div>
                        <div class="metric-label">æ£€ç´¢æ–‡æ¡£æ•°</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col3:
                    st.markdown(f"""
                    <div class="metric-card">
                        <div class="metric-value">{metadata_info.get('retrieved_doc_count_after_rerank', 'N/A')}</div>
                        <div class="metric-label">é‡æ’åºå</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                st.markdown("#### ğŸ“ ä¸Šä¸‹æ–‡é¢„è§ˆ")
                if len(retrieved_context_str) > 1000:
                    st.text_area(
                        "ä¸Šä¸‹æ–‡å†…å®¹",
                        retrieved_context_str[:1000] + "...",
                        height=200,
                        disabled=True,
                        key=f"context_{st.session_state.message_count}"
                    )
                else:
                    st.text_area(
                        "ä¸Šä¸‹æ–‡å†…å®¹",
                        retrieved_context_str,
                        height=200,
                        disabled=True,
                        key=f"context_{st.session_state.message_count}"
                    )
            
            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            if source_docs_for_ui:
                with st.expander("ğŸ“š å‚è€ƒæ¥æº", expanded=False):
                    for i, doc in enumerate(source_docs_for_ui[:5]):  # é™åˆ¶æ˜¾ç¤ºå‰5ä¸ªæ–‡æ¡£
                        source = doc.metadata.get('source', 'N/A')
                        page = doc.metadata.get('page', 'N/A')
                        category = doc.metadata.get('category', 'unknown')
                        
                        # æ–‡æ¡£å¡ç‰‡
                        st.markdown(f"""
                        <div class="info-card">
                            <h4>ğŸ“„ æ–‡æ¡£ {i+1}</h4>
                            <p><strong>æ¥æº:</strong> <code>{source}</code></p>
                            <p><strong>ç±»åˆ«:</strong> <span class="status-indicator status-info">{category}</span></p>
                            {f'<p><strong>é¡µç :</strong> {page}</p>' if page != 'N/A' else ''}
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # å†…å®¹é¢„è§ˆ
                        content_preview = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                        st.text_area(
                            f"å†…å®¹é¢„è§ˆ",
                            content_preview,
                            height=120,
                            disabled=True,
                            key=f"source_doc_preview_{st.session_state.message_count}_{i}"
                        )
                        
                        if i < len(source_docs_for_ui[:5]) - 1:
                            st.markdown("---")
            
        except Exception as e:
            st.error(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {e}")
            st.markdown(f"""
            <div class="chat-message assistant-message fade-in-up">
                <strong>ğŸ¤– åŠ©æ‰‹ï¼š</strong><br>
                æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†æŠ€æœ¯é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ï¼Œæˆ–è€…å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚
            </div>
            """, unsafe_allow_html=True)
    
    # å¢åŠ æ¶ˆæ¯è®¡æ•°å™¨
    st.session_state['message_count'] += 1

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style="text-align: center; padding: 2rem; color: var(--text-secondary);">
    <p>ğŸ“ <strong>CMU MISM RAG æ™ºèƒ½åŠ©æ‰‹</strong> | åŸºäºå…ˆè¿›çš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯</p>
    <p>ğŸ’¡ å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»ä½œè€…</p>
    <p>email:dwgqaz123@outlook.com</p>
    <p>github: <a href="https://github.com/DWGqaz123/cmu_mism_rag_assistant" target="_blank" style="color: var(--accent-blue);">DWGqaz123/cmu_mism_rag_assistant</a></p>
    <p style="font-size: 0.8rem; opacity: 0.7;">
        âš ï¸ æœ¬åŠ©æ‰‹æä¾›çš„ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç”³è¯·è¦æ±‚è¯·ä»¥å®˜æ–¹ç½‘ç«™ä¸ºå‡†
    </p>
</div>
""", unsafe_allow_html=True)

# æ·»åŠ è¿”å›é¡¶éƒ¨æŒ‰é’®
st.markdown("""
<style>
    .back-to-top {
        position: fixed;
        bottom: 20px;
        right: 20px;
        background: linear-gradient(135deg, var(--cmu-red) 0%, var(--cmu-dark-red) 100%);
        color: white;
        border: none;
        border-radius: 50%;
        width: 50px;
        height: 50px;
        font-size: 1.2rem;
        cursor: pointer;
        box-shadow: 0 4px 12px rgba(196, 30, 58, 0.3);
        transition: all 0.3s ease;
        z-index: 1000;
    }
    
    .back-to-top:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(196, 30, 58, 0.4);
    }
</style>

<button class="back-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">
    â†‘
</button>
""", unsafe_allow_html=True)